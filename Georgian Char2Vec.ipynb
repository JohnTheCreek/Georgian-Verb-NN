{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c154eb7d",
   "metadata": {},
   "source": [
    "# Georgian Char2Vec model \n",
    "\n",
    "This model will be trained on the kawiki wikimedia dump from the 21.03.2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74c9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(line):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', line)\n",
    "    # Collapse multiple whitespace to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove any character that is not a Georgian letter (ა-ჰ) or space\n",
    "    text = re.sub(r'[^ა-ჰ ]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "with open(r'C:\\Users\\Home\\Desktop\\Python Scripts\\wikipedia Char2vec\\kawiki-20250320-pages-articles-multistream.xml', 'r', encoding='utf-8') as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "cleaned_lines = [clean_text(line) for line in raw_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lines # preferably open in data wrangler or similar tool to check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequences = []\n",
    "for line in cleaned_lines:\n",
    "    # Skip empty lines\n",
    "    if not line:\n",
    "        continue\n",
    "    # Convert string to list of characters (spaces included)\n",
    "    chars = list(line)\n",
    "    char_sequences.append(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequences # check the output (again, preferably in a tool like data wrangler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360e8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can import gensim and create the char2vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=char_sequences,\n",
    "    vector_size=70,   # each character is embedded in 70-dimensional space\n",
    "    window=3,         # context window of ±3 characters\n",
    "    min_count=1,      # include all characters (no frequency cutoff)\n",
    "    sg=1,             # use skip-gram; sg=0 would be CBOW\n",
    "    workers=6         # use multiple CPU cores (adjust as needed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce42075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05419026,  0.00563378, -0.19373715, -0.04249081,  0.08144802,\n",
       "       -0.09836317,  0.04549257, -0.00763209, -0.01986822, -0.13298015,\n",
       "       -0.10904527, -0.10848953, -0.13864368,  0.22707045,  0.08945944,\n",
       "        0.05887409, -0.06608294, -0.03424963,  0.05651427, -0.07570385,\n",
       "        0.01690504, -0.23031797,  0.04436377,  0.07457335,  0.04419934,\n",
       "       -0.0659989 ,  0.04810683, -0.08581562, -0.07262397,  0.18199104,\n",
       "       -0.11170873,  0.09225684,  0.06508425,  0.04657187, -0.13099499,\n",
       "       -0.13851371,  0.10315397, -0.2060935 ,  0.17761321,  0.06892851,\n",
       "        0.03047237, -0.0087992 , -0.03969027,  0.0455033 , -0.08634393,\n",
       "        0.18564503,  0.0900536 ,  0.09447334, -0.04408414, -0.1128443 ,\n",
       "        0.11260456, -0.04557788,  0.05660466, -0.00816   ,  0.01893308,\n",
       "       -0.12012144, -0.04589818,  0.10544215,  0.07555658, -0.01709197,\n",
       "        0.08597674, -0.11429593,  0.34148946, -0.15637003,  0.03406613,\n",
       "       -0.33672574, -0.23496766,  0.10089198, -0.02178187, -0.0537529 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['ა'] # check the vector for the character 'ა'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b3f44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['ა'].shape # check the shape of the vector (should be (70,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3c8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# Get the embedding weight matrix (NumPy array) and convert to a PyTorch tensor\n",
    "weights = torch.FloatTensor(model.wv.vectors)\n",
    "torch.save(weights, 'char_embeddings.pt')    # save as a .pt file\n",
    "\n",
    "# Also save the mapping from character to index\n",
    "char_to_idx = {char: idx for idx, char in enumerate(model.wv.index_to_key)}\n",
    "with open('char2idx.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(char_to_idx, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
